{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nandini_TCSion_project.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ds3dgGSE9zhw"
      },
      "source": [
        "{\n",
        "  \"nbformat\": 4,\n",
        "  \"nbformat_minor\": 0,\n",
        "  \"metadata\": {\n",
        "    \"colab\": {\n",
        "      \"name\": \"Automate detection .ipynb\",\n",
        "      \"provenance\": [],\n",
        "      \"collapsed_sections\": []\n",
        "    },\n",
        "    \"kernelspec\": {\n",
        "      \"name\": \"python3\",\n",
        "      \"display_name\": \"Python 3\"\n",
        "    },\n",
        "    \"language_info\": {\n",
        "      \"name\": \"python\"\n",
        "    }\n",
        "  },\n",
        "  \"cells\": [\n",
        "    {\n",
        "      \"cell_type\": \"code\",\n",
        "      \"metadata\": {\n",
        "        \"colab\": {\n",
        "          \"base_uri\": \"https://localhost:8080/\",\n",
        "          \"height\": 609\n",
        "        },\n",
        "        \"id\": \"VpqvakLEs7Nu\",\n",
        "        \"outputId\": \"cb694e3d-588b-4c52-967d-6988727329a4\"\n",
        "      },\n",
        "      \"source\": [\n",
        "        \"import spacy \\n\",\n",
        "        \"from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS as stopwords \\n\",\n",
        "        \"from sklearn.feature_extraction.text import CountVectorizer \\n\",\n",
        "        \"from sklearn.metrics import accuracy_score \\n\",\n",
        "        \"from sklearn.base import TransformerMixin \\n\",\n",
        "        \"from sklearn.pipeline import Pipeline\\n\",\n",
        "        \"from sklearn.svm import LinearSVC\\n\",\n",
        "        \"from sklearn.metrics import confusion_matrix \\n\",\n",
        "        \"from sklearn.metrics import classification_report \\n\",\n",
        "        \"import string\\n\",\n",
        "        \"import matplotlib.pyplot as plt\\n\",\n",
        "        \"fig = plt.figure()\\n\",\n",
        "        \"import collections, numpy\\n\",\n",
        "        \"punctuations = string.punctuation\\n\",\n",
        "        \"!python3 -m spacy download en\\n\",\n",
        "        \"spacy.load('en_core_web_sm')\\n\",\n",
        "        \"from spacy.lang.en import English\\n\",\n",
        "        \"parser = English()\\n\",\n",
        "        \"import pandas as pd\\n\",\n",
        "        \"import random\\n\",\n",
        "        \"import inflect\\n\",\n",
        "        \"p = inflect.engine()\"\n",
        "      ],\n",
        "      \"execution_count\": null,\n",
        "      \"outputs\": [\n",
        "        {\n",
        "          \"output_type\": \"stream\",\n",
        "          \"name\": \"stderr\",\n",
        "          \"text\": [\n",
        "            \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.feature_extraction.stop_words module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.feature_extraction.text. Anything that cannot be imported from sklearn.feature_extraction.text is now part of the private API.\\n\",\n",
        "            \"  warnings.warn(message, FutureWarning)\\n\"\n",
        "          ]\n",
        "        },\n",
        "        {\n",
        "          \"output_type\": \"stream\",\n",
        "          \"name\": \"stdout\",\n",
        "          \"text\": [\n",
        "            \"Collecting en_core_web_sm==2.2.5\\n\",\n",
        "            \"  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz (12.0 MB)\\n\",\n",
        "            \"\\u001b[K     |████████████████████████████████| 12.0 MB 8.1 MB/s \\n\",\n",
        "            \"\\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\\n\",\n",
        "            \"Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.19.5)\\n\",\n",
        "            \"Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\\n\",\n",
        "            \"Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (57.4.0)\\n\",\n",
        "            \"Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\\n\",\n",
        "            \"Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\\n\",\n",
        "            \"Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.62.3)\\n\",\n",
        "            \"Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\\n\",\n",
        "            \"Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\\n\",\n",
        "            \"Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.5)\\n\",\n",
        "            \"Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\\n\",\n",
        "            \"Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.5)\\n\",\n",
        "            \"Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\\n\",\n",
        "            \"Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.8.2)\\n\",\n",
        "            \"Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (4.8.1)\\n\",\n",
        "            \"Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.6.0)\\n\",\n",
        "            \"Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.7.4.3)\\n\",\n",
        "            \"Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\\n\",\n",
        "            \"Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2021.5.30)\\n\",\n",
        "            \"Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\\n\",\n",
        "            \"Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.10)\\n\",\n",
        "            \"\\u001b[38;5;2m✔ Download and installation successful\\u001b[0m\\n\",\n",
        "            \"You can now load the model via spacy.load('en_core_web_sm')\\n\",\n",
        "            \"\\u001b[38;5;2m✔ Linking successful\\u001b[0m\\n\",\n",
        "            \"/usr/local/lib/python3.7/dist-packages/en_core_web_sm -->\\n\",\n",
        "            \"/usr/local/lib/python3.7/dist-packages/spacy/data/en\\n\",\n",
        "            \"You can now load the model via spacy.load('en')\\n\"\n",
        "          ]\n",
        "        },\n",
        "        {\n",
        "          \"output_type\": \"display_data\",\n",
        "          \"data\": {\n",
        "            \"text/plain\": [\n",
        "              \"<Figure size 432x288 with 0 Axes>\"\n",
        "            ]\n",
        "          },\n",
        "          \"metadata\": {}\n",
        "        }\n",
        "      ]\n",
        "    },\n",
        "    {\n",
        "      \"cell_type\": \"code\",\n",
        "      \"metadata\": {\n",
        "        \"id\": \"l7PPcBvutXTJ\"\n",
        "      },\n",
        "      \"source\": [\n",
        "        \"from google.colab import drive\\n\",\n",
        "        \"drive.mount('/content/drive')\"\n",
        "      ],\n",
        "      \"execution_count\": null,\n",
        "      \"outputs\": []\n",
        "    },\n",
        "    {\n",
        "      \"cell_type\": \"code\",\n",
        "      \"metadata\": {\n",
        "        \"id\": \"EwTG3KGwtidf\"\n",
        "      },\n",
        "      \"source\": [\n",
        "        \"def datapreprocessing(url):\\n\",\n",
        "        \"\\n\",\n",
        "        \"  # Load the dataset into a pandas dataframe.\\n\",\n",
        "        \"  df = pd.read_csv(url, delimiter='\\\\t', header=None, names=['sentence_source', 'label', 'label_notes', 'sentence'])\\n\",\n",
        "        \"\\n\",\n",
        "        \"  sentences = df.sentence.values\\n\",\n",
        "        \"  labels1 = df.label.values\\n\",\n",
        "        \"  # correct=labels.count(1)\\n\",\n",
        "        \"  \\n\",\n",
        "        \"  train=[]\\n\",\n",
        "        \"  actual_label=[]\\n\",\n",
        "        \"  for i in range(len(df)):\\n\",\n",
        "        \"    train.append((df.sentence[i],df.label[i]))\\n\",\n",
        "        \"    actual_label.append(df.label[i])\\n\",\n",
        "        \"  random.shuffle(train)\\n\",\n",
        "        \"\\n\",\n",
        "        \"  train_data = train[:7000]\\n\",\n",
        "        \"  test_data = train[7000:]\\n\",\n",
        "        \"  actual_labels=actual_label[7000:]  \\n\",\n",
        "        \"  return train_data,test_data,actual_labels,labels1\"\n",
        "      ],\n",
        "      \"execution_count\": null,\n",
        "      \"outputs\": []\n",
        "    },\n",
        "    {\n",
        "      \"cell_type\": \"code\",\n",
        "      \"metadata\": {\n",
        "        \"id\": \"GxkipJOCt50R\"\n",
        "      },\n",
        "      \"source\": [\n",
        "        \"train_data,test_data,labels,cor=datapreprocessing('/in_domain_train.tsv')\"\n",
        "      ],\n",
        "      \"execution_count\": null,\n",
        "      \"outputs\": []\n",
        "    },\n",
        "    {\n",
        "      \"cell_type\": \"code\",\n",
        "      \"metadata\": {\n",
        "        \"id\": \"04Pph5Kvt62G\"\n",
        "      },\n",
        "      \"source\": [\n",
        "        \"collections.Counter(labels)\"\n",
        "      ],\n",
        "      \"execution_count\": null,\n",
        "      \"outputs\": []\n",
        "    },\n",
        "    {\n",
        "      \"cell_type\": \"code\",\n",
        "      \"metadata\": {\n",
        "        \"id\": \"MMPes1AnuBH_\"\n",
        "      },\n",
        "      \"source\": [\n",
        "        \"\\n\",\n",
        "        \"fig = plt.figure()\\n\",\n",
        "        \"ax = fig.add_axes([0,0,1,1])\\n\",\n",
        "        \"classes = ['Grammatically Correct','Grammetically Incorret']\\n\",\n",
        "        \"students = [1100,451]\\n\",\n",
        "        \"ax.bar(classes,students)\\n\",\n",
        "        \"plt.show()\"\n",
        "      ],\n",
        "      \"execution_count\": null,\n",
        "      \"outputs\": []\n",
        "    },\n",
        "    {\n",
        "      \"cell_type\": \"code\",\n",
        "      \"metadata\": {\n",
        "        \"id\": \"Twu919Tjuc_7\"\n",
        "      },\n",
        "      \"source\": [\n",
        "        \"def model():\\n\",\n",
        "        \"  #Custom transformer using spaCy \\n\",\n",
        "        \"  class predictors(TransformerMixin):\\n\",\n",
        "        \"      def transform(self, X, **transform_params):\\n\",\n",
        "        \"          return [clean_text(text) for text in X]\\n\",\n",
        "        \"      def fit(self, X, y=None, **fit_params):\\n\",\n",
        "        \"          return self\\n\",\n",
        "        \"      def get_params(self, deep=True):\\n\",\n",
        "        \"          return {}\\n\",\n",
        "        \"\\n\",\n",
        "        \"  # Basic utility function to clean the text \\n\",\n",
        "        \"  def clean_text(text):     \\n\",\n",
        "        \"      return text.strip()\\n\",\n",
        "        \"  def spacy_tokenizer(sentence):\\n\",\n",
        "        \"      tokens = parser(sentence)\\n\",\n",
        "        \"      tokens = [ tok for tok in tokens]\\n\",\n",
        "        \"      return tokens\\n\",\n",
        "        \"\\n\",\n",
        "        \"  #create vectorizer object to generate feature vectors, we will use custom spacy tokenizer\\n\",\n",
        "        \"  vectorizer = CountVectorizer(tokenizer = spacy_tokenizer, ngram_range=(1,1)) \\n\",\n",
        "        \"  classifier = LinearSVC()\\n\",\n",
        "        \"\\n\",\n",
        "        \"  # Create the  pipeline to clean, tokenize, vectorize, and classify \\n\",\n",
        "        \"  pipe = Pipeline([(\\\"cleaner\\\", predictors()),('vectorizer', vectorizer),('classifier', classifier)])\\n\",\n",
        "        \"  \\n\",\n",
        "        \"  return pipe\"\n",
        "      ],\n",
        "      \"execution_count\": null,\n",
        "      \"outputs\": []\n",
        "    },\n",
        "    {\n",
        "      \"cell_type\": \"code\",\n",
        "      \"metadata\": {\n",
        "        \"id\": \"TTshI0-yu7Sp\"\n",
        "      },\n",
        "      \"source\": [\n",
        "        \"def detection(sen):\\n\",\n",
        "        \"  pipe=model()\\n\",\n",
        "        \"  pipe.fit([x[0] for x in train_data], [x[1] for x in train_data]) \\n\",\n",
        "        \"  pred_data = pipe.predict(sen) \\n\",\n",
        "        \"  result=pred_data[0]\\n\",\n",
        "        \"  return result\"\n",
        "      ],\n",
        "      \"execution_count\": null,\n",
        "      \"outputs\": []\n",
        "    },\n",
        "    {\n",
        "      \"cell_type\": \"code\",\n",
        "      \"metadata\": {\n",
        "        \"id\": \"VLVh_DVqvBWl\"\n",
        "      },\n",
        "      \"source\": [\n",
        "        \"pipe=model()\\n\",\n",
        "        \"def accuracy(test_data,actual_label):\\n\",\n",
        "        \"  pipe.fit([x[0] for x in train_data], [x[1] for x in train_data])\\n\",\n",
        "        \"  pred_data = pipe.predict([x[0] for x in test_data])\\n\",\n",
        "        \"  print ('Accuracy Score :',accuracy_score(labels, pred_data) )\\n\",\n",
        "        \"  \\n\",\n",
        "        \"  print ('Report : ')\\n\",\n",
        "        \"  print (classification_report(labels, pred_data) )\"\n",
        "      ],\n",
        "      \"execution_count\": null,\n",
        "      \"outputs\": []\n",
        "    },\n",
        "    {\n",
        "      \"cell_type\": \"code\",\n",
        "      \"metadata\": {\n",
        "        \"id\": \"OwF---cnvIzv\"\n",
        "      },\n",
        "      \"source\": [\n",
        "        \"def punct_data(sen):\\n\",\n",
        "        \"  # Load the dataset into a pandas dataframe.\\n\",\n",
        "        \"  df = pd.read_csv('/punct.csv')\\n\",\n",
        "        \"  \\n\",\n",
        "        \"  sentences = df.sentence.values\\n\",\n",
        "        \"  labels = df.lables.values\\n\",\n",
        "        \"  train=[]\\n\",\n",
        "        \"  for i in range(len(df)):\\n\",\n",
        "        \"    train.append((df.sentence[i],df.lables[i]))\\n\",\n",
        "        \"  \\n\",\n",
        "        \"  random.shuffle(train)\\n\",\n",
        "        \"\\n\",\n",
        "        \"  train_data = train[:200]\\n\",\n",
        "        \"  test_data = train[200:]  \\n\",\n",
        "        \"  class predictors(TransformerMixin):\\n\",\n",
        "        \"      def transform(self, X, **transform_params):\\n\",\n",
        "        \"          return [clean_text(text) for text in X]\\n\",\n",
        "        \"      def fit(self, X, y=None, **fit_params):\\n\",\n",
        "        \"          return self\\n\",\n",
        "        \"      def get_params(self, deep=True):\\n\",\n",
        "        \"          return {}\\n\",\n",
        "        \"  def clean_text(text):     \\n\",\n",
        "        \"      return text.strip().lower()\\n\",\n",
        "        \"  def spacy_tokenizer(sentence):\\n\",\n",
        "        \"      tokens = parser(sentence)\\n\",\n",
        "        \"      tokens = [ tok.lower_ for tok in tokens]\\n\",\n",
        "        \"      return tokens\\n\",\n",
        "        \"\\n\",\n",
        "        \"  #create vectorizer object to generate feature vectors, we will use custom spacy tokenizer\\n\",\n",
        "        \"  vectorizer = CountVectorizer(tokenizer = spacy_tokenizer, ngram_range=(1,1)) \\n\",\n",
        "        \"  classifier = LinearSVC()\\n\",\n",
        "        \"\\n\",\n",
        "        \"  # Create the  pipeline to clean, tokenize, vectorize, and classify \\n\",\n",
        "        \"  pipe = Pipeline([(\\\"cleaner\\\", predictors()),('vectorizer', vectorizer),('classifier', classifier)])\\n\",\n",
        "        \"  pipe.fit([x[0] for x in train_data], [x[1] for x in train_data]) \\n\",\n",
        "        \"  pred_data = pipe.predict(sen) \\n\",\n",
        "        \"  result=pred_data[0]\\n\",\n",
        "        \"  print(result)\\n\",\n",
        "        \"  return result\"\n",
        "      ],\n",
        "      \"execution_count\": null,\n",
        "      \"outputs\": []\n",
        "    },\n",
        "    {\n",
        "      \"cell_type\": \"code\",\n",
        "      \"metadata\": {\n",
        "        \"id\": \"cuksuc1MvsMF\"\n",
        "      },\n",
        "      \"source\": [\n",
        "        \"import spacy.cli\\n\",\n",
        "        \"spacy.cli.download(\\\"en_core_web_lg\\\")\"\n",
        "      ],\n",
        "      \"execution_count\": null,\n",
        "      \"outputs\": []\n",
        "    },\n",
        "    {\n",
        "      \"cell_type\": \"code\",\n",
        "      \"metadata\": {\n",
        "        \"id\": \"vLS19b4Fvwr-\"\n",
        "      },\n",
        "      \"source\": [\n",
        "        \"def article(sen):\\n\",\n",
        "        \"  import nltk\\n\",\n",
        "        \"  import en_core_web_lg\\n\",\n",
        "        \"  nlp = en_core_web_lg.load()\\n\",\n",
        "        \"  doc = nlp(sen)\\n\",\n",
        "        \"  art_word=[]\\n\",\n",
        "        \"  art=[]\\n\",\n",
        "        \"  fl=0\\n\",\n",
        "        \"  article=['a','an']\\n\",\n",
        "        \"  for i,word in enumerate(doc):\\n\",\n",
        "        \"    if(fl==1):\\n\",\n",
        "        \"      art_word.append(word)\\n\",\n",
        "        \"      fl=0\\n\",\n",
        "        \"    st=str(word)\\n\",\n",
        "        \"    if st in article:\\n\",\n",
        "        \"      art.append(st)\\n\",\n",
        "        \"      fl=1\\n\",\n",
        "        \"      pass\\n\",\n",
        "        \"  for i,word in enumerate(art_word):\\n\",\n",
        "        \"    st=str(word)\\n\",\n",
        "        \"    q=p.a(st)\\n\",\n",
        "        \"    if((art[i]+\\\" \\\"+st)==q):\\n\",\n",
        "        \"      return 1\\n\",\n",
        "        \"    else:\\n\",\n",
        "        \"      return 0\"\n",
        "      ],\n",
        "      \"execution_count\": null,\n",
        "      \"outputs\": []\n",
        "    },\n",
        "    {\n",
        "      \"cell_type\": \"code\",\n",
        "      \"metadata\": {\n",
        "        \"id\": \"kKruLM94v2Do\"\n",
        "      },\n",
        "      \"source\": [\n",
        "        \"def capital(sen):\\n\",\n",
        "        \"  if sen[0].islower():\\n\",\n",
        "        \"    print(\\\"Incorrect Capitalization\\\")\"\n",
        "      ],\n",
        "      \"execution_count\": null,\n",
        "      \"outputs\": []\n",
        "    },\n",
        "    {\n",
        "      \"cell_type\": \"code\",\n",
        "      \"metadata\": {\n",
        "        \"id\": \"UHeioa5kv7ew\"\n",
        "      },\n",
        "      \"source\": [\n",
        "        \"def check(sen):\\n\",\n",
        "        \"  result=detection(sen)\\n\",\n",
        "        \"  if result==1:\\n\",\n",
        "        \"    print(\\\"Sentence is Grammetically Correct\\\")\\n\",\n",
        "        \"  else:\\n\",\n",
        "        \"    print(\\\"Sentence is Grammetically Incorrect\\\")\\n\",\n",
        "        \"    punct=punctuation(sen)  \\n\",\n",
        "        \"    if punct==0:\\n\",\n",
        "        \"      print(\\\"Incorrect Punctuations\\\")\\n\",\n",
        "        \"    art=article(sen)  \\n\",\n",
        "        \"    if art==0:\\n\",\n",
        "        \"        print(\\\"Incorrect Article(a/an)\\\")  \\n\",\n",
        "        \"    capital(sen)\"\n",
        "      ],\n",
        "      \"execution_count\": null,\n",
        "      \"outputs\": []\n",
        "    },\n",
        "    {\n",
        "      \"cell_type\": \"code\",\n",
        "      \"metadata\": {\n",
        "        \"id\": \"I1kcnQ3iwAVD\"\n",
        "      },\n",
        "      \"source\": [\n",
        "        \"sen=[]\\n\",\n",
        "        \"sen.append(input(\\\"Enter Sentence\\\"))\\n\",\n",
        "        \"check(sen[0])\"\n",
        "      ],\n",
        "      \"execution_count\": null,\n",
        "      \"outputs\": []\n",
        "    },\n",
        "    {\n",
        "      \"cell_type\": \"code\",\n",
        "      \"metadata\": {\n",
        "        \"id\": \"dkeUqaCAwI9q\"\n",
        "      },\n",
        "      \"source\": [\n",
        "        \"accuracy(test_data,labels)\"\n",
        "      ],\n",
        "      \"execution_count\": null,\n",
        "      \"outputs\": []\n",
        "    }\n",
        "  ]\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}